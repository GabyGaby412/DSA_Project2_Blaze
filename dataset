#DATASET TOKENIZATION FOR DSA PROJECT 2
#make sure to run the steps in readme before running this code
import pandas as pd
import nltk
import re
from nltk.tokenize import word_tokenize
df = pd.read_csv('training.1600000.processed.noemoticon.csv', 
                 encoding='latin-1', 
                 header=None,
                 names=['sentiment', 'id', 'date', 'query', 'user', 'text'])

#print(df.head)

#nltk.download('punkt', download_dir='/Users/gabrielavelazquez/nltk_data')
# df["tokens"] = [word_tokenize(str(t).lower()) for t in df["text"]]
df["tokens"] = [re.findall(r"\b\w+\b", str(t).lower()) for t in df["text"]]

df.to_csv("tokenized_dataset.csv", index=False)
print("âœ… Tokenized words saved to tokenized_dataset.csv")
